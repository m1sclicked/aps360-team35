import tensorflow as tf
import numpy as np
import time

def gpu_functionality_test():
    # Check GPU availability
    print("GPU Devices:")
    gpus = tf.config.list_physical_devices('GPU')
    print(gpus)
    
    if not gpus:
        print("No GPU found. Ensure your GPU is properly configured.")
        return False
    
    # Test GPU computation
    print("\nRunning GPU Computation Test...")
    
    # Create a large matrix to perform computation
    with tf.device('/GPU:0'):
        # Create two large matrices
        start_time = time.time()
        matrix1 = tf.random.normal((10000, 10000))
        matrix2 = tf.random.normal((10000, 10000))
        
        # Perform matrix multiplication
        result = tf.matmul(matrix1, matrix2)
    
    # Measure computation time
    computation_time = time.time() - start_time
    print(f"Matrix Multiplication Completed in {computation_time:.4f} seconds")
    
    # Verify computation
    print("\nComputation Verification:")
    print(f"Result Shape: {result.shape}")
    print(f"Result Data Type: {result.dtype}")
    
    return True

def compare_cpu_gpu_performance():
    print("\nCPU vs GPU Performance Comparison:")
    
    # Large matrix sizes for comparison
    sizes = [1000, 5000, 10000]
    
    for size in sizes:
        print(f"\nMatrix Size: {size}x{size}")
        
        # CPU Computation
        start_time_cpu = time.time()
        with tf.device('/CPU:0'):
            matrix_cpu1 = tf.random.normal((size, size))
            matrix_cpu2 = tf.random.normal((size, size))
            result_cpu = tf.matmul(matrix_cpu1, matrix_cpu2)
        cpu_time = time.time() - start_time_cpu
        print(f"CPU Computation Time: {cpu_time:.4f} seconds")
        
        # GPU Computation
        start_time_gpu = time.time()
        with tf.device('/GPU:0'):
            matrix_gpu1 = tf.random.normal((size, size))
            matrix_gpu2 = tf.random.normal((size, size))
            result_gpu = tf.matmul(matrix_gpu1, matrix_gpu2)
        gpu_time = time.time() - start_time_gpu
        print(f"GPU Computation Time: {gpu_time:.4f} seconds")
        
        # Speedup calculation
        speedup = cpu_time / gpu_time if gpu_time > 0 else 0
        print(f"GPU Speedup: {speedup:.2f}x")

def main():
    print("TensorFlow GPU Functionality Test")
    print("================================")
    
    # TensorFlow and GPU Information
    print(f"TensorFlow Version: {tf.__version__}")
    print(f"NumPy Version: {np.__version__}")
    
    # Run tests
    gpu_test_passed = gpu_functionality_test()
    
    if gpu_test_passed:
        compare_cpu_gpu_performance()
    
    print("\nTest Complete!")

if __name__ == '__main__':
    main()